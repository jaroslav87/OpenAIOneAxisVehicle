{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gym_stableBaseline3_moving_mass_DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaroslav87/OpenAIOneAxisVehicle/blob/master/gym_stableBaseline3_moving_mass_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp8rSS4DIhEV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "eb5d1893-9877-47d3-d189-283af93c7d89"
      },
      "source": [
        "# Stable Baselines 3 for tnesorflow 2.x\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "#!pip install stable-baselines[mpi]==2.10.0\n",
        "\n",
        "!pip install stable-baselines3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (0.17.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (1.0.5)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (1.6.0+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (1.18.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.17->stable-baselines3) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.17->stable-baselines3) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines3) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines3) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable-baselines3) (0.16.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->stable-baselines3) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl7gyUrrpRPH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "70d806af-80db-4933-b453-0d89646b5927"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print((np.random.rand(1)-0.5).item()*100.0)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-38.6080788678007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYzDXA9vJfz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "\n",
        "class MovingMassEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Custom Environment that follows gym interface.\n",
        "  This is a simple env where the force is acting on moving mass. \n",
        "  The agent must learn how to use the force to reach the set position on X axis. \n",
        "  It is assumed that only by using forces +1 -1 and 0 N we can \n",
        "  \"\"\"\n",
        "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "  metadata = {'render.modes': ['console']}\n",
        " \n",
        "\n",
        "  def __init__(self, grid_size=100):\n",
        "    super(MovingMassEnv, self).__init__()\n",
        "\n",
        "    self.stepp = 0\n",
        "    # Size of the 1D-grid\n",
        "    self.grid_size = grid_size\n",
        "    self.grid_size_max = grid_size*10\n",
        "    # Initialize the agent at the right of the grid with zero vel and acc\n",
        "    self.agent_pos = grid_size - 20\n",
        "    self.agent_v = 0.0 #speed equals to zero\n",
        "    self.agent_a = 0.0  # accelertion equal to zero\n",
        "\n",
        "    #model/sim params:\n",
        "    self.m = 1.0 # in kg\n",
        "    self.delta = 0.05 # friction coefficient\n",
        "    self.dt = 0.1 # in s\n",
        "    self.max_v = 10 # m/s\n",
        "    self.f_max = 5 # N\n",
        "\n",
        "    # Define action and observation space\n",
        "    # They must be gym.spaces objects\n",
        "    # Example when using discrete actions, we have three: zero, left and right\n",
        "    n_actions = 3\n",
        "    action_high = np.array([self.f_max]) #f\n",
        "    action_low = np.array([-self.f_max])\n",
        "    self.action_space = spaces.Box(low=action_low, \n",
        "                                        high=action_high)\n",
        "    # The observation will be the coordinate of the agent/model and its speed\n",
        "    # this can be described both by Discrete and Box space\n",
        "    observation_high = np.array([self.grid_size_max, self.max_v]) #pos\n",
        "    observation_low = np.array([-self.grid_size_max, -self.max_v])\n",
        "\n",
        "    self.observation_space = spaces.Box(low=observation_low, \n",
        "                                        high=observation_high)\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Important: the observation must be a numpy array\n",
        "    :return: (np.array) \n",
        "    \"\"\"\n",
        "    # Initialize the agent at the right of the grid with zero vel and acc\n",
        "    self.agent_pos = self.grid_size-120\n",
        "    self.agent_v = 0.0\n",
        "    self.agent_a = 0.0\n",
        "\n",
        "    self.stepp = 0\n",
        "\n",
        "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
        "    return np.array([self.agent_pos, self.agent_v]).astype(np.float32)\n",
        "\n",
        "  def step(self, action, set_pos = 0):\n",
        "    \n",
        "    f = action[0]\n",
        "\n",
        "    # model execution:\n",
        "    ff = self.delta * self.agent_v # friction force\n",
        "    self.agent_a = (f - ff)/self.m # acceleartion\n",
        "    self.agent_v = self.agent_v + self.agent_a * self.dt\n",
        "    self.agent_pos = self.agent_pos + self.agent_v * self.dt\n",
        "    # Account for the boundaries of the grid and vel\n",
        "    self.agent_pos = np.clip(self.agent_pos,-self.grid_size, self.grid_size)\n",
        "    self.agent_v = np.clip(self.agent_v, -self.max_v, self.max_v)\n",
        "\n",
        "    # Are we at the left of the grid?\n",
        "    target = set_pos#(np.random.rand(1)-0.5).item()*self.grid_size\n",
        "    d = np.abs(self.agent_pos-target)\n",
        "    v = np.abs(self.agent_v)\n",
        "\n",
        "    done = bool(d < 0.01 and v < 0.01)\n",
        "\n",
        "    # reward\n",
        "    \n",
        "    reward =-d #- self.stepp/10000#- v# - self.stepp/10000\n",
        "    \n",
        "    self.stepp += 1\n",
        "    # Optionally we can pass additional info, we are not using that for now\n",
        "    info = {}\n",
        "\n",
        "    return np.array([self.agent_pos, self.agent_v]).astype(np.float32), reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "    # agent is represented as a cross, rest as a dot\n",
        "    #print(\".\" * self.agent_pos, end=\"\")\n",
        "    #print(\"x\", end=\"\")\n",
        "    #print(\".\" * (self.grid_size - self.agent_pos))\n",
        "\n",
        "  def close(self):\n",
        "    pass\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy5mlho1-Ine",
        "colab_type": "text"
      },
      "source": [
        "### Validate the environment\n",
        "\n",
        "Stable Baselines provides a [helper](https://stable-baselines.readthedocs.io/en/master/common/env_checker.html) to check that your environment follows the Gym interface. It also optionally checks that the environment is compatible with Stable-Baselines (and emits warning if necessary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DOpP_B0-LXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from stable_baselines3.common.env_checker import check_env"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CcUVatq-P0l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "88b3b2c6-00be-4776-fb94-aa8cf876a30d"
      },
      "source": [
        "env = MovingMassEnv()\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/stable_baselines3/common/env_checker.py:225: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
            "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3khFtkSE0g",
        "colab_type": "text"
      },
      "source": [
        "### Testing the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i62yf2LvSAYY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ffa88a88-a857-457d-9c46-2a51eafb992b"
      },
      "source": [
        "env = MovingMassEnv(grid_size=100)\n",
        "\n",
        "obs = env.reset()\n",
        "env.render()\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "print(env.action_space.sample())\n",
        "\n",
        "force = [-0.5]\n",
        "# Hardcoded best agent: always go left!\n",
        "n_steps = 50\n",
        "for step in range(n_steps):\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  obs, reward, done, info = env.step(force, 20)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render()\n",
        "  if done:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box(2,)\n",
            "Box(1,)\n",
            "[1.2466291]\n",
            "Step 1\n",
            "obs= [-20.005  -0.05 ] reward= -40.004999999999995 done= False\n",
            "Step 2\n",
            "obs= [-20.014975  -0.09975 ] reward= -40.014975 done= False\n",
            "Step 3\n",
            "obs= [-20.0299      -0.14925125] reward= -40.029900125 done= False\n",
            "Step 4\n",
            "obs= [-20.049751  -0.198505] reward= -40.049750624375 done= False\n",
            "Step 5\n",
            "obs= [-20.074501    -0.24751247] reward= -40.074501871253126 done= False\n",
            "Step 6\n",
            "obs= [-20.10413    -0.2962749] reward= -40.10412936189686 done= False\n",
            "Step 7\n",
            "obs= [-20.138609    -0.34479353] reward= -40.13860871508737 done= False\n",
            "Step 8\n",
            "obs= [-20.177916    -0.39306957] reward= -40.17791567151194 done= False\n",
            "Step 9\n",
            "obs= [-20.222027   -0.4411042] reward= -40.22202609315438 done= False\n",
            "Step 10\n",
            "obs= [-20.270916   -0.4888987] reward= -40.270915962688605 done= False\n",
            "Step 11\n",
            "obs= [-20.324562   -0.5364542] reward= -40.32456138287516 done= False\n",
            "Step 12\n",
            "obs= [-20.382938    -0.58377194] reward= -40.38293857596079 done= False\n",
            "Step 13\n",
            "obs= [-20.446024    -0.63085306] reward= -40.44602388308098 done= False\n",
            "Step 14\n",
            "obs= [-20.513794   -0.6776988] reward= -40.51379376366558 done= False\n",
            "Step 15\n",
            "obs= [-20.586226    -0.72431034] reward= -40.586224794847254 done= False\n",
            "Step 16\n",
            "obs= [-20.663294   -0.7706888] reward= -40.66329367087302 done= False\n",
            "Step 17\n",
            "obs= [-20.744978    -0.81683534] reward= -40.744977202518655 done= False\n",
            "Step 18\n",
            "obs= [-20.831253   -0.8627511] reward= -40.83125231650606 done= False\n",
            "Step 19\n",
            "obs= [-20.922096   -0.9084374] reward= -40.92209605492353 done= False\n",
            "Step 20\n",
            "obs= [-21.017485   -0.9538952] reward= -41.017485574648916 done= False\n",
            "Step 21\n",
            "obs= [-21.117397   -0.9991257] reward= -41.11739814677567 done= False\n",
            "Step 22\n",
            "obs= [-21.221811   -1.0441301] reward= -41.22181115604179 done= False\n",
            "Step 23\n",
            "obs= [-21.330702   -1.0889094] reward= -41.330702100261576 done= False\n",
            "Step 24\n",
            "obs= [-21.444048   -1.1334649] reward= -41.44404858976027 done= False\n",
            "Step 25\n",
            "obs= [-21.561829   -1.1777976] reward= -41.56182834681147 done= False\n",
            "Step 26\n",
            "obs= [-21.68402    -1.2219086] reward= -41.68401920507741 done= False\n",
            "Step 27\n",
            "obs= [-21.810598  -1.265799] reward= -41.81059910905202 done= False\n",
            "Step 28\n",
            "obs= [-21.941545  -1.30947 ] reward= -41.94154611350676 done= False\n",
            "Step 29\n",
            "obs= [-22.076838   -1.3529227] reward= -42.07683838293923 done= False\n",
            "Step 30\n",
            "obs= [-22.216454   -1.3961581] reward= -42.21645419102453 done= False\n",
            "Step 31\n",
            "obs= [-22.360373   -1.4391773] reward= -42.36037192006941 done= False\n",
            "Step 32\n",
            "obs= [-22.50857    -1.4819814] reward= -42.50857006046906 done= False\n",
            "Step 33\n",
            "obs= [-22.661028   -1.5245715] reward= -42.661027210166715 done= False\n",
            "Step 34\n",
            "obs= [-22.817722   -1.5669487] reward= -42.81772207411588 done= False\n",
            "Step 35\n",
            "obs= [-22.978634   -1.6091139] reward= -42.9786334637453 done= False\n",
            "Step 36\n",
            "obs= [-23.14374    -1.6510683] reward= -43.14374029642657 done= False\n",
            "Step 37\n",
            "obs= [-23.31302   -1.692813] reward= -43.313021594944445 done= False\n",
            "Step 38\n",
            "obs= [-23.486456   -1.7343489] reward= -43.486456486969715 done= False\n",
            "Step 39\n",
            "obs= [-23.664024   -1.7756772] reward= -43.66402420453487 done= False\n",
            "Step 40\n",
            "obs= [-23.845705   -1.8167988] reward= -43.845704083512196 done= False\n",
            "Step 41\n",
            "obs= [-24.031475   -1.8577148] reward= -44.03147556309463 done= False\n",
            "Step 42\n",
            "obs= [-24.221317   -1.8984262] reward= -44.22131818527916 done= False\n",
            "Step 43\n",
            "obs= [-24.41521    -1.9389341] reward= -44.41521159435277 done= False\n",
            "Step 44\n",
            "obs= [-24.613136   -1.9792395] reward= -44.613135536381 done= False\n",
            "Step 45\n",
            "obs= [-24.81507    -2.0193431] reward= -44.8150698586991 done= False\n",
            "Step 46\n",
            "obs= [-25.020994   -2.0592465] reward= -45.0209945094056 done= False\n",
            "Step 47\n",
            "obs= [-25.23089    -2.0989504] reward= -45.23088953685857 done= False\n",
            "Step 48\n",
            "obs= [-25.444735   -2.1384556] reward= -45.44473508917428 done= False\n",
            "Step 49\n",
            "obs= [-25.662512   -2.1777632] reward= -45.66251141372841 done= False\n",
            "Step 50\n",
            "obs= [-25.8842     -2.2168744] reward= -45.884198856659765 done= False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv1e1qJETfHU",
        "colab_type": "text"
      },
      "source": [
        "### Try it with Stable-Baselines\n",
        "\n",
        "Once your environment follow the gym interface, it is quite easy to plug in any algorithm from stable-baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQfLBE28SNDr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "bc6c6a6e-8894-4437-d3c1-269bd398a842"
      },
      "source": [
        "from stable_baselines3 import DQN, A2C, DDPG, PPO\n",
        "from stable_baselines3.common.cmd_util import make_vec_env\n",
        "\n",
        "# Instantiate the env\n",
        "env = MovingMassEnv(grid_size=100)\n",
        "# wrap it\n",
        "#env = make_vec_env(lambda: env, n_envs=1)\n",
        "env.configure(m = 2.0)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-39105c624269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# wrap it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#env = make_vec_env(lambda: env, n_envs=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'MovingMassEnv' object has no attribute 'configure'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRV4Q7FVUKB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the agent\n",
        "model = PPO('MlpPolicy', env, n_steps=256, verbose=0).learn(102400) #ACKTR('MlpPolicy', env, verbose=1).learn(100000)\n",
        "#DDPG('MlpPolicy', env, verbose=1).learn(100000)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJbeiF0RUN-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "3dc95596-786e-41ed-f9e4-0b47ec27c8ce"
      },
      "source": [
        "# Test the trained agent\n",
        "\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "obs = env.reset()\n",
        "n_steps = 9000\n",
        "pos_agent = []\n",
        "v_agent = []\n",
        "pos_time = []\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  #print(\"Step {}\".format(step + 1))\n",
        "  #print(\"Action: \", action)\n",
        "  obs, reward, done, info = env.step(action,0)\n",
        "  pos_agent.append(obs[0])\n",
        "  v_agent.append(obs[1])\n",
        "  pos_time.append(step + 1)\n",
        "  if step>8900:\n",
        "    print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render(mode='console')\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break\n",
        "\n",
        "plt.plot(pos_time, pos_agent, 'r')\n",
        "plt.title('Agent position')\n",
        "plt.xlabel(\"steps\")\n",
        "plt.ylabel(\"Position\")\n",
        "plt.legend([\"Pos\"])\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(pos_time, v_agent, 'r')\n",
        "plt.title('Agent vel')\n",
        "plt.xlabel(\"steps\")\n",
        "plt.ylabel(\"Velocity\")\n",
        "plt.legend([\"vel\"])\n",
        "\n",
        "plt.figure()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-401293f3a106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m#print(\"Step {}\".format(step + 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m#print(\"Action: \", action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mpos_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mv_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: step() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    }
  ]
}